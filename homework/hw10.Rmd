---
title: "Homework #10: Networks" 
author: "**Your Name Here**"
date: "Due: Tue Dec 7 | 3:25pm"
output: R6030::homework
---

**DS 6030 | Fall 2021 | University of Virginia**

------------------------------------------------------------------------

```{r config, echo=FALSE}
source(system.file("config/hw_config.R", package="R6030")) # knitr settings
options(dplyr.summarise.inform = FALSE)  # ignore dplyr message about grouping
```

# Required R packages and Directories {-}

::: {.solution}
```{r packages, message=FALSE, warning=FALSE}
data.dir = 'https://mdporter.github.io/DS6030/data/' # data directory
library(R6030)     # functions for DS-6030
library(tidyverse) # functions for data manipulation  
library(igraph)    # functions for graph analytics   
```
:::



# Problem 1 The Marvel Universe

[Alberich, Miro-Julia, \& Rossell&oacute; (2002)](https://arxiv.org/pdf/cond-mat/0202174.pdf) 
examined the social network structure of the Marvel Comics Universe and found some similarities to real-world collaboration networks. 
The folks at <http://syntagmatic.github.io/exposedata/marvel/> have made the network data available (along with some nice visualizations). I have extracted the Hero Social Network Data which can be accessed using [this link](https://mdporter.github.io/DS6030/data/marvel_hero-network.csv). Note that the data contain one edge for each time two heroes appeared in the same comic.  


## a. Load the data and make a *weighted* and *undirected* graph, where the `weight` corresponds to the number of times the heroes appeared in the same comic. 
- Ensure your graph has an edge attribute named `weight`. The weight between *LITTLE, ABNER* and *BLACK PANTHER/T'CHAL* should be 7. 
- No need to make a plot, just show your code to make the graph object.

::: {.solution}
Add Solution Here
:::



## b. Run the *fast-greedy* community detection algorithm (`igraph::cluster_fast_greedy()`).
- Use the edge weights in the community detection algorithm.
- How many communities did it find? 
- Use a plot to show community size of each group (i.e., group number on the x-axis and group size on y-axis).

::: {.solution}
Add Solution Here
:::


## c. Calculate the following centrality scores for the hero network: *eigenvector, betweeness, and degree*. 
- `igraph` has two versions of centrality calculations (I know, a bit confusing).
- The ones starting with `centr_` do not consider edge weights.
- The others (e.g., `betweenness()`, `eigen_centrality()`) will allow weights.
- For this exercise, ignore the weights and use the `centr_` versions. 
- Show the top 10 heroes arranged by *eigenvector centrality*. 
- Which hero has the largest eigenvector centrality? How does this make the hero *important*? 

::: {.solution}
Add Solution Here
:::

## d. For each of the three largest communities find the hero with the largest *betweeness centrality*. Explain how these heroes are *important*. 

::: {.solution}
Add Solution Here
:::

       

# Problem 2: Alpha Centrality


[Bonacich and Lloyd (2001)](https://github.com/mdporter/DS6030/raw/master/other/alpha-centrality_Bonacich.pdf) introduced *alpha centrality* as an alternative to eigenvector centrality. Their main idea is that the importance of a node is based on the network structure **plus** some known external sources of importance. The alpha centrality vector $x$ is defined:
\[ 
x = \alpha  A^T x + s
\]
where $s$ is the vector of exogenous importance and $0 \leq \alpha \leq 1/\lambda_1$ (where $\lambda_1$ is the maximum eigenvalue of $A$) reflects the relative importance of the endogenous factors of importance.


## a. PageRank can be considered a special case of alpha centrality. What does PageRank use for $s$, $\alpha$, and $A$? Use the notation from the class notes,  e.g., $\alpha=d$. 
::: {.solution}
Add Solution Here
:::


---

::: {style="background-color:lightgrey; display: block; border-color: black; padding:1em"}

The next few problems will explore how alpha centrality can be used for identifying the bad actors in the money laundering data. The money laundering data was used in class and can be accessed here:

- nodes: <https://mdporter.github.io/DS6030/data/transfers_nodes.csv>

- edges: <https://mdporter.github.io/DS6030/data/transfers.csv>

:::


## b. Make a *directed* graph from these data. 
- Show code, no need to make a plot
- Note: the `time` column may cause a message when you create the igraph object. We don't use time for this problem, so it can be safely ignored.  

::: {.solution}
Add Solution Here
:::

## c. Using the *directed graph*, set $s=1$ for the known fraudsters, $s=0$ for the legitimate, and $s=0.01$ for the unknown nodes and calculate the alpha centrality. You can think of $s$ as proportional to the prior probability that a node is a fraudster. 
- Use $\alpha = 0.8$. 
- Use a Cleveland dot plot (or bar plot) to visually display the alpha centrality scores for all node. Use color (or shape) to distinguish between the fraud, non-fraud, and unknown nodes. 
- Comment on what this tells you about the two unknown nodes

::: {.solution}
Add Solution Here
:::


# Problem 3: Hubs and Authorities (HITS) 

The HITS algorithm is described in [MMDS 5.5](http://infolab.stanford.edu/~ullman/mmds/ch5.pdf)


## a. The HITS scores are designed to work with *directed* networks. What is the result of running HITS on an *undirected* network? Show that the scores reduce to a familiar centrality score. 

::: {.solution}
Add Solution Here
:::

## b. Write a function to calculate the Hubs and Authority scores. See MMDS 5.5.2 for details. 

::: {.solution}
Add Solution Here
:::

## c. Use your function to calculate the hubs and authority scores for the Political Blog data [Adamic and Glance (2005). "The political blogosphere and the 2004 U.S. election: divided they blog", In Proceedings of the *3rd International Workshop on Link discovery (LinkKDD '05)*. ACM, New York, NY, USA](https://mdporter.github.io/DS6030/other/(Adamic)%20Political%20Blogs.pdf).

- nodes: <https://mdporter.github.io/DS6030/data/polblogs_nodes.csv>

- edges: <https://mdporter.github.io/DS6030/data/polblogs.csv>

The `nodes` data has a column named `leaning` which indicates the political leaning (liberal or conservative) of the blog and a column named `label` which gives the blog name. 

Run HITS on the full data, and then report the top 5 hubs and top 5 authority scores (with blog name) for both liberal and conservative blogs.

- In the case of a failure in part b, use the igraph functions, e.g. `hub.score()` and `authority.score()`.   
- Note: the network is *directed*
 

::: {.solution}
Add Solution Here
:::






 


